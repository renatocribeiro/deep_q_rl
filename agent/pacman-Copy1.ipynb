{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Authors: Aleksey Grinchuk (AlexGrinch), Mariya Popova (Mariewelt) ... and some hedgehog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=cpu'\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function \n",
    "experiment_setup_name = \"tutorial.gym.atari.MsPacman-ram-v0.rnn\"\n",
    "\n",
    "\n",
    "#gym game title\n",
    "GAME_TITLE = 'MsPacman-ram-v0'\n",
    "\n",
    "#how many parallel game instances can your machine tolerate\n",
    "N_PARALLEL_GAMES = 1\n",
    "\n",
    "#how long is one replay session from a batch\n",
    "#since we have window-like memory (no recurrent layers), we can use relatively small session weights\n",
    "replay_seq_len = 50\n",
    "\n",
    "\n",
    "#theano device selection. GPU is, as always, in preference, but not required\n",
    "%env THEANO_FLAGS='device=cpu'\n",
    "\n",
    "#number of bytes in RAM\n",
    "RAM_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is a showcase on how to use AgentNet for OpenAI Gym environments\n",
    "\n",
    "\n",
    "* Pacman game as an example\n",
    "* Training a simple lasagne neural network for Q_learning objective\n",
    " * This example can be easily modified to use more difficult convolutional networks and/or recurrent agent memory. \n",
    " \n",
    "* Training via simple experience replay (explained below)\n",
    "* Only using utility recurrent layers for simplicity of this example\n",
    " * but adding a few RNNs or GRUs shouldn's be a problem\n",
    "* the network is trained with a simple ten-step Q-learning for simplicity\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "  \n",
    "### Installing it\n",
    " * If nothing changed on their side, to run this, you bacically need to follow their install instructions - \n",
    " \n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .[all]\n",
    "```\n",
    "\n",
    "## New to AgentNet and Lasagne?\n",
    "* This is pretty much the basic tutorial for AgentNet, so it's okay not to know it.\n",
    "* We only require surface level knowledge of theano and lasagne, so you can just learn them as you go.\n",
    "* Alternatively, you can find Lasagne tutorials here:\n",
    " * Official mnist example: http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    " * From scratch: https://github.com/ddtm/dl-course/tree/master/Seminar4\n",
    " * From theano: https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb\n",
    "\n",
    "\n",
    "## The library\n",
    "\n",
    "In this notebook we shall use [AgentNet](https://github.com/BladeCarrier/AgentNet/) library.\n",
    "Agentnet, in essence, is an additional kit of lasagne layers that allow you to build custom recurrent layers.\n",
    "Assuming you already have Bleeding Edge theano and lasagne, you can install it via\n",
    "```\n",
    "git clone https://github.com/yandexdataschool/AgentNet\n",
    "cd AgentNet\n",
    "python setup.py install\n",
    "```\n",
    "in whatever python, environment or container you exist. Alternatively, see docker install instructions in the [readme](https://github.com/yandexdataschool/AgentNet/blob/master/README.md).\n",
    "\n",
    "\n",
    "Depending what python version do you use, in may be \n",
    "* `python3 setup.py install` \\ `python2 setup.py install` if you are using a different python\n",
    "* add sudo - `sudo python setup.py install` - if you have a superuser-installed python\n",
    "* in case you have any problems - contact us or consider using a docker container (see above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us to preprocess original images from size of 210x160x3 (RGB images) to size of 110x84 (grayscale images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess(image):\n",
    "    def_h = 84\n",
    "    def_w = 110\n",
    "    img = 0.299*image[:,:,0] + 0.587*image[:,:,1] + 0.114*image[:,:,2]\n",
    "    img = cv2.resize(img, (def_h, def_w))\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_tensor(tensor):\n",
    "    def_h = 84\n",
    "    def_w = 110\n",
    "    tnsr = np.zeros((tensor.shape[0], tensor.shape[1], def_w, def_h, 1))\n",
    "    for i in xrange(tensor.shape[0]):\n",
    "        for j in xrange(tensor.shape[1]):\n",
    "            img = 0.299*tensor[i,j,:,:,0] + 0.587*tensor[i,j,:,:,1] + 0.114*tensor[i,j,:,:,2]\n",
    "            tnsr[i,j,:,:,0] = cv2.resize(img, (def_h, def_w))\n",
    "    return tnsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_tensor4(tensor):\n",
    "    def_h = 84\n",
    "    def_w = 110\n",
    "    tnsr = np.zeros((tensor.shape[0], def_w, def_h, 1))\n",
    "    for i in xrange(tensor.shape[0]):\n",
    "        img = 0.299*tensor[i,:,:,0] + 0.587*tensor[i,:,:,1] + 0.114*tensor[i,:,:,2]\n",
    "        tnsr[i,:,:,0] = cv2.resize(img, (def_h, def_w))\n",
    "    return tnsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-10 13:54:58,367] You have 'numpy' version 1.8.2 installed, but 'gym' requires at least 1.10.4. HINT: upgrade via 'pip install -U numpy'.\n",
      "[2016-07-10 13:54:58,390] Making new env: MsPacman-ram-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3d82051f90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM8AAAEACAYAAAAUSCKKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYTwCgUH+P8m0MbhUVw/QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d8219db90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "atari = gym.make(GAME_TITLE)\n",
    "atari.reset()\n",
    "plt.imshow(atari.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3d6846f390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM8AAAD/CAYAAABW+4LyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqfKIBFLlEQmkyiMSSJVHJND/A+REnUgJYe3wAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d8237e990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(atari.render('rgb_array'))[:,:,0], cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Parameters\n",
    "* observation dimensions, actions, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n"
     ]
    }
   ],
   "source": [
    "n_actions = atari.action_space.n\n",
    "observation_shape = (None,) + atari.observation_space.shape #(110, 84, 1)\n",
    "action_names = atari.get_action_meanings()\n",
    "print action_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent setup step by step\n",
    "* An agent implementation may contain these parts:\n",
    " * Observation(s)\n",
    "   * InputLayers where observed game states (here - images) are sent at each tick \n",
    " * Memory layer(s)\n",
    "   * A dictionary that maps \"New memory layers\" to \"prev memory layers\"\n",
    " * Policy layer (e.g. Q-values or probabilities)\n",
    "   * in this case, a lasagne dense layer based on observation layer\n",
    " * Resolver - acton picker layer\n",
    "   * chooses what action to take given Q-values\n",
    "   * in this case, the resolver has epsilon-greedy policy\n",
    "   \n",
    "   \n",
    "We are going to build something of this shape:\n",
    "\n",
    "(one can assume that the 'time' goes from left to right, inputs are at the bottom and outputs go to the top)\n",
    "\n",
    "\n",
    "\n",
    "![window_dqn_scheme](http://s32.postimg.org/yy5q3wadx/window_dqn.png)\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent observations\n",
    "\n",
    "* Here you define where observations (game images) appear in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DimshuffleLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#image observation at current tick goes here\n",
    "observation_layer = InputLayer(observation_shape, name=\"RAM input\")\n",
    "\n",
    "\n",
    "#reshape to [batch, color, x, y] to allow for convolutional layers to work correctly\n",
    "observation_reshape = observation_layer # DimshuffleLayer(observation_layer,(0,3,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### Agent memory states\n",
    " * Here you can define arbitrary transitions between \"previous state\" variables and their next states\n",
    " * The rules are\n",
    "   * previous states must be input layers\n",
    "   * next states must have same shape as previous ones\n",
    "   * otherwise it can be any lasagne network\n",
    "   * AgentNet.memory has several useful layers\n",
    "   \n",
    " * During training and evaluation, your states will be updated recurrently\n",
    "   * next state at t=1 is given as previous state to t=2\n",
    " \n",
    " * Finally, you have to define a dictionary mapping new state -> previous state\n",
    "\n",
    "\n",
    "### In this demo\n",
    "Since we have almost fully observable environment AND we want to keep baseline simple, we shall use no recurrent units.\n",
    "However, Atari game environments are known to have __flickering__ effect where some sprites are shown only on odd frames and others on even ones - that was used to optimize performance at the time.\n",
    "To compensate for this, we shall use the memory layer called __WindowAugmentation__ which basically maintains a K previous time steps of what it is fed with.\n",
    "\n",
    "One can try to use\n",
    " * GRU - `from agentnet.memory import GRUMemoryLayer`\n",
    " * RNN - `from agentnet.memory import RNNCell`\n",
    " * any custom lasagne layers that compute new memory states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#memory\n",
    "#using simple window-based memory that stores several states\n",
    "#the SpaceInvaders environment does not need any more as it is almost fully-observed\n",
    "from agentnet.memory import WindowAugmentation\n",
    "\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "\n",
    "#prev state input\n",
    "prev_window = InputLayer((None,window_size)+tuple(observation_reshape.output_shape[1:]),\n",
    "                        name = \"previous window state\")\n",
    "\n",
    "\n",
    "#our window\n",
    "window = WindowAugmentation(observation_reshape,\n",
    "                            prev_window,\n",
    "                            name = \"new window state\")\n",
    "\n",
    "\n",
    "\n",
    "memory_dict = {window:prev_window}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural network body\n",
    "Our strategy, again:\n",
    " * take pixel-wise maximum over the window\n",
    " * apply some layers\n",
    " * use output layer to predict Q-values(see next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import DropoutLayer,DenseLayer, ExpressionLayer\n",
    "\"\"\"\n",
    "#you may use any other lasagne layers, including convolutions, batch_norms, maxout, etc\n",
    "\n",
    "#pixel-wise maximum over the temporal window (to avoid flickering)\n",
    "window_max = ExpressionLayer(window,\n",
    "                             lambda a: a.max(axis=1),\n",
    "                             output_shape = (None,)+window.output_shape[2:])\n",
    "\n",
    "\n",
    "\n",
    "#a simple lasagne network (try replacing with any other lasagne network and see what works best)    \n",
    "nn = lasagne.layers.Conv2DLayer(window_max, num_filters=16, filter_size=(8, 8), stride=(4, 4))\n",
    "nn = lasagne.layers.BatchNormLayer(nn)\n",
    "nn = lasagne.layers.Conv2DLayer(nn, num_filters=32, filter_size=(4, 4), stride=(2, 2))\n",
    "nn = lasagne.layers.BatchNormLayer(nn)\n",
    "nn = lasagne.layers.DenseLayer(nn, num_units=256)\n",
    "nn = lasagne.layers.BatchNormLayer(nn)\n",
    "\"\"\"\n",
    "#WARNING! if your network is computing too slowly, try decreasing the amount of neurons\n",
    "\n",
    "\n",
    "hidden_layer_1 = lasagne.layers.DenseLayer(observation_layer,\n",
    "                                           num_units=RAM_SIZE,\n",
    "                                           nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                           W=lasagne.init.HeUniform(),\n",
    "                                           b=lasagne.init.Constant(.1))\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(hidden_layer_1,\n",
    "                                           num_units=RAM_SIZE,\n",
    "                                           nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                           W=lasagne.init.HeUniform(),\n",
    "                                           b=lasagne.init.Constant(.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent policy and action picking\n",
    "* Since we are training a deep Q-network, we need it to predict Q-values and take actions.\n",
    "* Hence we define a lasagne layer that is used for action output\n",
    "\n",
    "* To pick actions, we use an epsilon-greedy resolver\n",
    "  * Note that resolver outputs particular action IDs and not probabilities.\n",
    "  * These actions are than sent into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#q_eval\n",
    "q_eval = DenseLayer(nn,\n",
    "                   num_units = n_actions,\n",
    "                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                   name=\"QEvaluator\")\n",
    "\n",
    "#resolver\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "resolver = EpsilonGreedyResolver(q_eval,name=\"resolver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layer,\n",
    "              {}, #memory_dict,\n",
    "              q_eval,resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, QEvaluator.W, QEvaluator.b]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent step function\n",
    "* computes action and next state given observation and prev state\n",
    "* written in a generic way to support any recurrences, windows, LTMs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile theano graph for one step decision making\n",
    "applier_fun = agent.get_react_function()\n",
    "\n",
    "#a nice pythonic interface\n",
    "\"\"\"\n",
    "def step(observation, prev_memories = 'zeros', batch_size = N_PARALLEL_GAMES):\n",
    "    # returns actions and new states given observation and prev state\n",
    "    # Prev state in default setup should be [prev window,]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #default to zeros\n",
    "    if prev_memories == 'zeros':\n",
    "        prev_memories = [np.zeros((batch_size,)+tuple(mem.output_shape[1:]),\n",
    "                                  dtype='float32') \n",
    "                         for mem in agent.state_variables]\n",
    "    \n",
    "    obs = preprocess_tensor4(np.array(observation))\n",
    "    obs = obs.astype(np.float32, copy=False)\n",
    "    res = applier_fun(obs,*prev_memories)\n",
    "    \n",
    "    action = res[0]\n",
    "    memories = res[1:]\n",
    "    return action, memories\n",
    "\"\"\"\n",
    "    \n",
    "def step(observation, prev_memories='zeros', batch_size=N_PARALLEL_GAMES):\n",
    "    \"\"\" returns actions and new states given observation and prev state\n",
    "    Prev state in default setup should be [prev window,]\"\"\"\n",
    "    #default to zeros\n",
    "    if prev_memories == 'zeros':\n",
    "        prev_memories = [np.zeros((batch_size,)+tuple(mem.output_shape[1:]),\n",
    "                         dtype='float32') for mem in agent.agent_states]\n",
    "    #print(\"prev memories:\", prev_memories)\n",
    "    res = applier_fun(np.array(observation), *prev_memories)\n",
    "    action = res[0]\n",
    "    memories = res[1:]\n",
    "#    print(\"new memories:\", memories)\n",
    "    return action, memories\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* We define a small container that stores\n",
    " * game emulators\n",
    " * last agent observations\n",
    " * agent memories at last time tick\n",
    "* This allows us to instantly continue a session from where it stopped\n",
    "\n",
    "\n",
    "\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-10 14:02:05,047] Making new env: MsPacman-ram-v0\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import GamePool\n",
    "\n",
    "pool = GamePool(GAME_TITLE, N_PARALLEL_GAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 0 ns, total: 32 ms\n",
      "Wall time: 33.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "observation_log,action_log,reward_log,_,_,_  = pool.interact(step,50)\n",
    "\n",
    "#print(np.array(action_names)[np.array(action_log)[:3,:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience replay pool\n",
    "\n",
    "Since our network exists in a theano graph and OpenAI gym doesn't, we shall train out network via experience replay.\n",
    "\n",
    "To do that in AgentNet, one can use a SessionPoolEnvironment.\n",
    "\n",
    "It's simple: you record new sessions using `interact(...)`, and than immediately train on them.\n",
    "\n",
    "1. Interact with Atari, get play sessions\n",
    "2. Store them into session environment\n",
    "3. Train on them\n",
    "4. Repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "env = SessionPoolEnvironment(observations = observation_layer,\n",
    "                             actions=resolver,\n",
    "                             agent_memories=agent.state_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def update_pool(env, pool,n_steps=100):\n",
    "    # a function that creates new sessions and ads them into the pool\n",
    "    # throwing the old ones away entirely for simplicity\n",
    "\n",
    "    preceding_memory_states = list(pool.prev_memory_states)\n",
    "    \n",
    "    #get interaction sessions\n",
    "    observation_tensor,action_tensor,reward_tensor,_,is_alive_tensor,_= pool.interact(step,n_steps=n_steps)\n",
    "    observation_tensor = preprocess_tensor(observation_tensor)\n",
    "    \n",
    "    #load them into experience replay environment\n",
    "    env.load_sessions(observation_tensor,action_tensor,reward_tensor,is_alive_tensor,preceding_memory_states)\n",
    "\"\"\"\n",
    "    \n",
    "def update_pool(env, pool, n_steps=replay_seq_len):\n",
    "    preceding_memory_states = list(pool.prev_memory_states)\n",
    "\n",
    "    #get interaction sessions\n",
    "    observation_tensor, action_tensor, reward_tensor, memory_logs, is_alive_tensor, _ = pool.interact(step, n_steps=n_steps)\n",
    "\n",
    "    \"\"\"print(preceding_memory_states)\n",
    "    print(\"here:\")\n",
    "    print(env.preceding_agent_memories)\n",
    "    print(len(env.preceding_agent_memories))\n",
    "    print(len(preceding_memory_states))\n",
    "    print(memory_logs)\n",
    "    print(memory_logs[0].shape)\"\"\"\n",
    "\n",
    "    #load them into experience replay environment\n",
    "    print(\"obstensor shape: \", observation_tensor[0].shape)\n",
    "    print(\"obs len:\", len(env.observations), \"obs shape: \", env.observations[0].get_value().shape)\n",
    "\n",
    "    env.append_sessions(observation_tensor/128., action_tensor, reward_tensor,\n",
    "                        is_alive_tensor, preceding_memory_states, max_pool_size=1e4)  # TODO: what does it mean?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('obstensor shape: ', (50, 128))\n",
      "('obs len:', 1, 'obs shape: ', (10, 5, 128))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henryk/AgentNet/agentnet/environment/session_pool.py:286: UserWarning: Warning! Appending sessions to empty or broken pool. Old pool sessions, if any, are disposed.\n",
      "  warn(\"Warning! Appending sessions to empty or broken pool. Old pool sessions, if any, are disposed.\")\n"
     ]
    }
   ],
   "source": [
    "#load first  sessions\n",
    "update_pool(env,pool,replay_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated way of training is to store a large pool of sessions and train on random batches of them. \n",
    "* Why that is expected to be better - http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\n",
    "* Or less proprietary - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "To do that, one might make use of\n",
    "* ```env.load_sessions(...)``` - load new sessions\n",
    "* ```env.get_session_updates(...)``` - does the same thing via theano updates (advanced)\n",
    "* ```batch_env = env.sample_session_batch(batch_size, ...)``` - create an experience replay environment that contains batch_size random sessions from env (rerolled each time). Should be used in training instead of env.\n",
    "* ```env.select_session_batch(indices)``` does the same thing deterministically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via experience replay\n",
    "\n",
    "* We use agent we have created to replay environment interactions inside Theano\n",
    "* to than train on the replayed sessions via theano gradient propagation\n",
    "* this is essentially basic Lasagne code after the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "_,_,_,_,qvalues_seq = agent.get_sessions(\n",
    "    env,\n",
    "    session_length=replay_seq_len,\n",
    "    batch_size=env.batch_size,\n",
    "    optimize_experience_replay=True,\n",
    ")\n",
    "\n",
    "\n",
    "#The \"_\"s are\n",
    "#first - environment states - which is empty since we are using session pool as our environment\n",
    "#secund - observation sequences - whatever agent recieved at observation input(s) on each tick\n",
    "#third - a dictionary of all agent memory units (RNN, GRU, NTM) - empty as we use none of them\n",
    "#last - \"imagined\" actions - actions agent would pick now if he was in that situation \n",
    "#                              - irrelevant since we are replaying and not actually playing the game now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* In this part we are using some basic Reinforcement Learning methods (here - Q-learning) to train\n",
    "* AgentNet has plenty of such methods, but we shall use the simple Q_learning for now.\n",
    "* Later you can try:\n",
    " * SARSA - simpler on-policy algorithms\n",
    " * N-step q-learning (requires n_steps parameter)\n",
    " * Advantage Actor-Critic (requires state values and probabilities instead of Q-values)\n",
    "\n",
    "\n",
    "* The basic interface is .get_elementwise_objective \n",
    "  * it returns loss function (here - squared error against reference Q-values) values at each batch and tick\n",
    "  \n",
    "* If you want to do it the hard way instead, try .get_reference_Qvalues and compute errors on ya own\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "from agentnet.learning import qlearning_n_step\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#IMPORTANT!\n",
    "# If you are training on a game that has rewards far outside some [-5,+5]\n",
    "# it is a good idea to downscale them to avoid divergence\n",
    "scaled_reward_seq = env.rewards\n",
    "#For SpaceInvaders, however, not scaling rewards is at least working\n",
    "\n",
    "\n",
    "\"\"\"elwise_mse_loss = qlearning_n_step.get_elementwise_objective(qvalues_seq,\n",
    "                                                        env.actions[0],\n",
    "                                                        scaled_reward_seq,\n",
    "                                                        env.is_alive,\n",
    "                                                        n_steps=10,\n",
    "                                                        gamma_or_gammas=0.99,)\"\"\"\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                        env.actions[0],\n",
    "                                                        scaled_reward_seq,\n",
    "                                                        env.is_alive,\n",
    "                                                        gamma_or_gammas=0.99,)\n",
    "\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "mse_loss = elwise_mse_loss.sum() / env.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = mse_loss + reg_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.adadelta(loss,weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mean session reward\n",
    "mean_session_reward = env.rewards.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "evaluation_fun = theano.function([],[loss,mse_loss,reg_l2,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session visualization tools\n",
    "\n",
    "Just a helper function that draws current game images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_sessions(max_n_sessions = 3):\n",
    "    \"\"\"just draw random images\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=[15,8])\n",
    "    \n",
    "    pictures = [atari.render(\"rgb_array\") for atari in pool.games[:max_n_sessions]]\n",
    "    for i,pic in enumerate(pictures):\n",
    "        plt.subplot(1,max_n_sessions,i+1)\n",
    "        plt.imshow(pic)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "display_sessions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 10000\n",
    "#25k may take hours to train.\n",
    "#consider interrupt early.\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    update_pool(env,pool,replay_seq_len)\n",
    "    resolver.rng.seed(i)    \n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    if epoch_counter%1 ==0:\n",
    "        current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/1000.)\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%10 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        resolver.epsilon.set_value(0)\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "\n",
    "        print(\"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy))\n",
    "        print(\"rec %.3f reg %.3f\"%(q_loss,l2_penalty))\n",
    "\n",
    "    if epoch_counter %100 ==0:\n",
    "        print(\"Learning curves:\")\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print(\"Random session examples\")\n",
    "        display_sessions()\n",
    "    \n",
    "    #run several sessions of game, record videos and save obtained results\n",
    "    if epoch_counter %1000 ==0:\n",
    "        \n",
    "        save_path = 'videos/MSPacman-v0_' + str(epoch_counter)\n",
    "\n",
    "        subm_env = gym.make(GAME_TITLE)\n",
    "\n",
    "        #starting monitor. This setup does not write videos\n",
    "        #subm_env.monitor.start(save_path,lambda i: False,force=True)\n",
    "\n",
    "        #this setup does\n",
    "        subm_env.monitor.start(save_path,force=True)\n",
    "\n",
    "        rws = []\n",
    "\n",
    "        for i_episode in xrange(250):\n",
    "\n",
    "            #initial observation\n",
    "            observation = subm_env.reset()\n",
    "            #initial memory\n",
    "            prev_memories = \"zeros\"\n",
    "\n",
    "            s_reward =0.\n",
    "            t = 0\n",
    "            while True:\n",
    "\n",
    "                action,new_memories = step([observation],prev_memories,batch_size=1)\n",
    "                observation, reward, done, info = subm_env.step(action[0])\n",
    "\n",
    "                s_reward += reward\n",
    "\n",
    "                prev_memories = new_memories\n",
    "                if done:\n",
    "                    print(\"Episode finished after {} timesteps, rw = {}\".format(t+1,s_reward))\n",
    "                    rws.append(s_reward)\n",
    "                    break\n",
    "                t+=1\n",
    "\n",
    "        subm_env.monitor.close()\n",
    "\n",
    "        rws = np.array(rws)\n",
    "        np.savez(open('rws4hist2_'+str(epoch_counter)+'.npz', 'wb'), rws=rws)\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_path = 'videos/MSPacman-v0_' + str(epoch_counter-1)\n",
    "\n",
    "subm_env = gym.make(GAME_TITLE)\n",
    "\n",
    "#starting monitor. This setup does not write videos\n",
    "#subm_env.monitor.start(save_path,lambda i: False,force=True)\n",
    "\n",
    "#this setup does\n",
    "subm_env.monitor.start(save_path,force=True)\n",
    "\n",
    "rws = []\n",
    "\n",
    "for i_episode in xrange(220):\n",
    "\n",
    "    #initial observation\n",
    "    observation = subm_env.reset()\n",
    "    #initial memory\n",
    "    prev_memories = \"zeros\"\n",
    "\n",
    "    s_reward =0.\n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        action,new_memories = step([observation],prev_memories,batch_size=1)\n",
    "        observation, reward, done, info = subm_env.step(action[0])\n",
    "\n",
    "        s_reward += reward\n",
    "\n",
    "        prev_memories = new_memories\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps, rw = {}\".format(t+1,s_reward))\n",
    "            rws.append(s_reward)\n",
    "            break\n",
    "        t+=1\n",
    "\n",
    "subm_env.monitor.close()\n",
    "\n",
    "rws = np.array(rws)\n",
    "np.savez(open('rws4hist_'+str(epoch_counter-1)+'.npz', 'wb'), rws=rws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(rws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Random session examples\")\n",
    "display_sessions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "lc = {'e-greedy':score_log['expected e-greedy reward'], 'greedy':score_log['expected greedy reward']}\n",
    "\n",
    "with open(str(epoch_counter-1)+'iter_lc.pickle', 'wb') as handle:\n",
    "    lc = pickle.dump(lc, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission\n",
    "Here we simply run the OpenAI gym submission code and view scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resolver.epsilon.set_value(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_path = '/tmp/AgentNet-simplenet-MsPacman-v0-Recording0'\n",
    "\n",
    "subm_env = gym.make(GAME_TITLE)\n",
    "\n",
    "#starting monitor. This setup does not write videos\n",
    "subm_env.monitor.start(save_path,lambda i: False,force=True)\n",
    "\n",
    "#this setup does\n",
    "#subm_env.monitor.start(save_path,force=True)\n",
    "\n",
    "\n",
    "for i_episode in xrange(200):\n",
    "    \n",
    "    #initial observation\n",
    "    observation = subm_env.reset()\n",
    "    #initial memory\n",
    "    prev_memories = \"zeros\"\n",
    "    \n",
    "    \n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        action,new_memories = step([observation],prev_memories,batch_size=1)\n",
    "        observation, reward, done, info = subm_env.step(action[0])\n",
    "        \n",
    "        prev_memories = new_memories\n",
    "        if done:\n",
    "            print \"Episode finished after {} timesteps\".format(t+1)\n",
    "            break\n",
    "        t+=1\n",
    "\n",
    "subm_env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[monitor.close() for monitor in gym.monitoring._open_monitors()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "gym.upload(save_path,\n",
    "           \n",
    "           #this notebook\n",
    "           writeup=<url to my gist>, \n",
    "           \n",
    "           #your api key\n",
    "           api_key=<my_own_api_key>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Once you got it working,\n",
    "Try building a network that maximizes the final score\n",
    "\n",
    "* Moar lasagne stuff: convolutional layers, batch normalization, nonlinearities and so on\n",
    "* Recurrent agent memory layers, GRUMemoryLayer, etc\n",
    "* Different reinforcement learning algorithm (p.e. agent-critic), other parameters\n",
    "* Experience replay pool\n",
    "\n",
    "\n",
    "Look for examples? Try examples/Deep Kung Fu for most of these features\n",
    "\n",
    "\n",
    "You can also try to expand to a different game: \n",
    " * all OpenAI Atari games are already compatible, you only need to change GAME_TITLE\n",
    " * Other discrete action space environments are also accessible this way\n",
    " * For continuous action spaces, either discretize actions or use continuous RL algorithms (e.g. .learning.dpg_n_step)\n",
    " * Adapting to a custom non-OpenAI environment can be done with a simple wrapper\n",
    "\n",
    " \n",
    "__Good luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gym.upload?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
