{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "experiment_setup_name = \"tutorial.gym.atari.spaceinvaders-v0.cnn\"\n",
    "\n",
    "\n",
    "#gym game title\n",
    "GAME_TITLE = 'SpaceInvaders-v0'\n",
    "\n",
    "#how many parallel game instances can your machine tolerate\n",
    "N_PARALLEL_GAMES = 5\n",
    "\n",
    "#how long is one replay session from a batch\n",
    "#since we have window-like memory (no recurrent layers), we can use relatively small session weights\n",
    "replay_seq_len = 10\n",
    "\n",
    "\n",
    "#theano device selection. GPU is, as always, in preference, but not required\n",
    "%env THEANO_FLAGS='device=cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is a showcase on how to use AgentNet for OpenAI Gym environments\n",
    "\n",
    "\n",
    "* Space Invaders game as an example\n",
    "* Training a simple lasagne neural network for Q_learning objective\n",
    " * This example can be easily modified to use convolutional networks and/or recurrent agent memory. \n",
    " \n",
    "* Training via simple experience replay (explained below)\n",
    "* Only using utility recurrent layers for simplicity of this example\n",
    " * but adding a few RNNs or GRUs shouldn's be a problem\n",
    "* the network is trained with a simple one-step Q-learning for simplicity\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "  \n",
    "### Installing it\n",
    " * If nothing changed on their side, to run this, you bacically need to follow their install instructions - \n",
    " \n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .[all]\n",
    "```\n",
    "\n",
    "## New to AgentNet and Lasagne?\n",
    "* This is pretty much the basic tutorial for AgentNet, so it's okay not to know it.\n",
    "* We only require surface level knowledge of theano and lasagne, so you can just learn them as you go.\n",
    "* Alternatively, you can find Lasagne tutorials here:\n",
    " * Official mnist example: http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    " * From scratch: https://github.com/ddtm/dl-course/tree/master/Seminar4\n",
    " * From theano: https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "atari = gym.make(GAME_TITLE)\n",
    "atari.reset()\n",
    "plt.imshow(atari.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Parameters\n",
    "* observation dimensions, actions, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_actions = atari.action_space.n\n",
    "observation_shape = (None,)+atari.observation_space.shape\n",
    "action_names = atari.get_action_meanings()\n",
    "print(action_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup step by step\n",
    "* An agent implementation may contain these parts:\n",
    " * Observation(s)\n",
    "   * InputLayers where observed game states (here - images) are sent at each tick \n",
    " * Memory layer(s)\n",
    "   * A dictionary that maps \"New memory layers\" to \"prev memory layers\"\n",
    " * Policy layer (e.g. Q-values or probabilities)\n",
    "   * in this case, a lasagne dense layer based on observation layer\n",
    " * Resolver - acton picker layer\n",
    "   * chooses what action to take given Q-values\n",
    "   * in this case, the resolver has epsilon-greedy policy\n",
    "   \n",
    "   \n",
    "We are going to build something of this shape:\n",
    "\n",
    "(one can assume that the 'time' goes from left to right, inputs are at the bottom and outputs go to the top)\n",
    "\n",
    "\n",
    "\n",
    "![window_dqn_scheme](http://s32.postimg.org/yy5q3wadx/window_dqn.png)\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent observations\n",
    "\n",
    "* Here you define where observations (game images) appear in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DimshuffleLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#image observation at current tick goes here\n",
    "observation_layer = InputLayer(observation_shape,\n",
    "                               name=\"images input\")\n",
    "\n",
    "\n",
    "#reshape to [batch, color, x, y] to allow for convolutional layers to work correctly\n",
    "observation_reshape = DimshuffleLayer(observation_layer,(0,3,1,2))\n",
    "\n",
    "observation_reshape = lasagne.layers.Pool2DLayer(observation_reshape,(2,2),mode='average_inc_pad')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### Agent memory states\n",
    " * Here you can define arbitrary transitions between \"previous state\" variables and their next states\n",
    " * The rules are\n",
    "   * previous states must be input layers\n",
    "   * next states must have same shape as previous ones\n",
    "   * otherwise it can be any lasagne network\n",
    "   * AgentNet.memory has several useful layers\n",
    "   \n",
    " * During training and evaluation, your states will be updated recurrently\n",
    "   * next state at t=1 is given as previous state to t=2\n",
    " \n",
    " * Finally, you have to define a dictionary mapping new state -> previous state\n",
    "\n",
    "\n",
    "### In this demo\n",
    "Since we have almost fully observable environment AND we want to keep baseline simple, we shall use no recurrent units.\n",
    "However, Atari game environments are known to have __flickering__ effect where some sprites are shown only on odd frames and others on even ones - that was used to optimize performance at the time.\n",
    "To compensate for this, we shall use the memory layer called __WindowAugmentation__ which basically maintains a K previous time steps of what it is fed with.\n",
    "\n",
    "One can try to use\n",
    " * GRU - `from agentnet.memory import GRUMemoryLayer`\n",
    " * RNN - `from agentnet.memory import RNNCell`\n",
    " * any custom lasagne layers that compute new memory states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#memory\n",
    "#using simple window-based memory that stores several states\n",
    "#the SpaceInvaders environment does not need any more as it is almost fully-observed\n",
    "from agentnet.memory import WindowAugmentation\n",
    "\n",
    "#we are planning something else\n",
    "from agentnet.memory import RNNCell\n",
    "\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "\n",
    "#prev state input\n",
    "prev_window = InputLayer((None,window_size)+tuple(observation_reshape.output_shape[1:]),\n",
    "                        name = \"previous window state\")\n",
    "\n",
    "\n",
    "#our window\n",
    "window = WindowAugmentation(observation_reshape,\n",
    "                            prev_window,\n",
    "                            name = \"new window state\")\n",
    "\n",
    "\n",
    "\n",
    "memory_dict = {window:prev_window}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural network body\n",
    "Our strategy, again:\n",
    " * take pixel-wise maximum over the window\n",
    " * apply some layers\n",
    " * use output layer to predict Q-values(see next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import DropoutLayer,DenseLayer, ExpressionLayer\n",
    "\n",
    "\"\"\" Old stuff\n",
    "you may use any other lasagne layers, including convolutions, batch_norms, maxout, etc\n",
    "\n",
    "#pixel-wise maximum over the temporal window (to avoid flickering)\n",
    "window_max = ExpressionLayer(window,\n",
    "                             lambda a: a.max(axis=1),\n",
    "                             output_shape = (None,)+window.output_shape[2:])\n",
    "\n",
    "\n",
    "\n",
    "#a simple lasagne network (try replacing with any other lasagne network and see what works best)    \n",
    "nn = lasagne.layers.Conv2DLayer(window_max,32,filter_size=8,stride=(4,4), name='cnn0')\n",
    "nn = lasagne.layers.Conv2DLayer(nn,64,filter_size=8,stride=(4,4), name='cnn1')\n",
    "\n",
    "#nn = DropoutLayer(nn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "nn = DenseLayer(nn,num_units=256,name='dense1')\n",
    "\n",
    "\n",
    "#WARNING! if your network is computing too slowly, try decreasing the amount of neurons\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "obs_layer = InputLayer(observation_shape, name=\"observation layer\")\n",
    "\n",
    "hidden_layer_1 = lasagne.layers.DenseLayer(obs_layer,\n",
    "                                           num_units=RAM_SIZE,\n",
    "                                           nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                           W=lasagne.init.HeUniform(),\n",
    "                                           b=lasagne.init.Constant(.1))\n",
    "\n",
    "hidden_layer_2 = lasagne.layers.DenseLayer(hidden_layer_1,\n",
    "                                           num_units=RAM_SIZE,\n",
    "                                           nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                           W=lasagne.init.HeUniform(),\n",
    "                                           b=lasagne.init.Constant(.1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent policy and action picking\n",
    "* Since we are training a deep Q-network, we need it to predict Q-values and take actions.\n",
    "* Hence we define a lasagne layer that is used for action output\n",
    "\n",
    "* To pick actions, we use an epsilon-greedy resolver\n",
    "  * Note that resolver outputs particular action IDs and not probabilities.\n",
    "  * These actions are than sent into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#q_eval\n",
    "q_eval = DenseLayer(nn,\n",
    "                   num_units = n_actions,\n",
    "                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                   name=\"QEvaluator\")\n",
    "\n",
    "#resolver\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "resolver = EpsilonGreedyResolver(q_eval,name=\"resolver\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layer,\n",
    "              memory_dict,\n",
    "              q_eval,resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent step function\n",
    "* computes action and next state given observation and prev state\n",
    "* written in a generic way to support any recurrences, windows, LTMs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile theano graph for one step decision making\n",
    "applier_fun = agent.get_react_function()\n",
    "\n",
    "#a nice pythonic interface\n",
    "def step(observation, prev_memories = 'zeros',batch_size = N_PARALLEL_GAMES):\n",
    "    \"\"\" returns actions and new states given observation and prev state\n",
    "    Prev state in default setup should be [prev window,]\"\"\"\n",
    "    #default to zeros\n",
    "    if prev_memories == 'zeros':\n",
    "        prev_memories = [np.zeros((batch_size,)+tuple(mem.output_shape[1:]),\n",
    "                                  dtype='float32') \n",
    "                         for mem in agent.agent_states]\n",
    "    res = applier_fun(np.array(observation),*prev_memories)\n",
    "    action = res[0]\n",
    "    memories = res[1:]\n",
    "    return action,memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* We define a small container that stores\n",
    " * game emulators\n",
    " * last agent observations\n",
    " * agent memories at last time tick\n",
    "* This allows us to instantly continue a session from where it stopped\n",
    "\n",
    "\n",
    "\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.experiments.openai_gym.pool import GamePool\n",
    "\n",
    "pool = GamePool(GAME_TITLE, N_PARALLEL_GAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "observation_log,action_log,reward_log,_,_,_  = pool.interact(step,50)\n",
    "\n",
    "print(np.array(action_names)[np.array(action_log)[:3,:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experience replay pool\n",
    "\n",
    "Since our network exists in a theano graph and OpenAI gym doesn't, we shall train out network via experience replay.\n",
    "\n",
    "To do that in AgentNet, one can use a SessionPoolEnvironment.\n",
    "\n",
    "It's simple: you record new sessions using `interact(...)`, and than immediately train on them.\n",
    "\n",
    "1. Interact with Atari, get play sessions\n",
    "2. Store them into session environment\n",
    "3. Train on them\n",
    "4. Repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "env = SessionPoolEnvironment(observations = observation_layer,\n",
    "                             actions=resolver,\n",
    "                             agent_memories=agent.agent_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_pool(env, pool,n_steps=100):\n",
    "    \"\"\" a function that creates new sessions and ads them into the pool\n",
    "    throwing the old ones away entirely for simplicity\"\"\"\n",
    "\n",
    "    preceding_memory_states = list(pool.prev_memory_states)\n",
    "    \n",
    "    #get interaction sessions\n",
    "    observation_tensor,action_tensor,reward_tensor,_,is_alive_tensor,_= pool.interact(step,n_steps=n_steps)\n",
    "        \n",
    "    #load them into experience replay environment\n",
    "    env.load_sessions(observation_tensor,action_tensor,reward_tensor,is_alive_tensor,preceding_memory_states)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first  sessions\n",
    "update_pool(env,pool,replay_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated way of training is to store a large pool of sessions and train on random batches of them. \n",
    "* Why that is expected to be better - http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\n",
    "* Or less proprietary - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "To do that, one might make use of\n",
    "* ```env.load_sessions(...)``` - load new sessions\n",
    "* ```env.get_session_updates(...)``` - does the same thing via theano updates (advanced)\n",
    "* ```batch_env = env.sample_session_batch(batch_size, ...)``` - create an experience replay environment that contains batch_size random sessions from env (rerolled each time). Should be used in training instead of env.\n",
    "* ```env.select_session_batch(indices)``` does the same thing deterministically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via experience replay\n",
    "\n",
    "* We use agent we have created to replay environment interactions inside Theano\n",
    "* to than train on the replayed sessions via theano gradient propagation\n",
    "* this is essentially basic Lasagne code after the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "_,_,_,_,qvalues_seq = agent.get_sessions(\n",
    "    env,\n",
    "    session_length=replay_seq_len,\n",
    "    batch_size=env.batch_size,\n",
    "    optimize_experience_replay=True,\n",
    ")\n",
    "\n",
    "\n",
    "#The \"_\"s are\n",
    "#first - environment states - which is empty since we are using session pool as our environment\n",
    "#secund - observation sequences - whatever agent recieved at observation input(s) on each tick\n",
    "#third - a dictionary of all agent memory units (RNN, GRU, NTM) - empty as we use none of them\n",
    "#last - \"imagined\" actions - actions agent would pick now if he was in that situation \n",
    "#                              - irrelevant since we are replaying and not actually playing the game now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* In this part we are using some basic Reinforcement Learning methods (here - Q-learning) to train\n",
    "* AgentNet has plenty of such methods, but we shall use the simple Q_learning for now.\n",
    "* Later you can try:\n",
    " * SARSA - simpler on-policy algorithms\n",
    " * N-step q-learning (requires n_steps parameter)\n",
    " * Advantage Actor-Critic (requires state values and probabilities instead of Q-values)\n",
    "\n",
    "\n",
    "* The basic interface is .get_elementwise_objective \n",
    "  * it returns loss function (here - squared error against reference Q-values) values at each batch and tick\n",
    "  \n",
    "* If you want to do it the hard way instead, try .get_reference_Qvalues and compute errors on ya own\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#IMPORTANT!\n",
    "# If you are training on a game that has rewards far outside some [-5,+5]\n",
    "# it is a good idea to downscale them to avoid divergence\n",
    "scaled_reward_seq = env.rewards\n",
    "#For SpaceInvaders, however, not scaling rewards is at least working\n",
    "\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                        env.actions[0],\n",
    "                                                        scaled_reward_seq,\n",
    "                                                        env.is_alive,\n",
    "                                                        gamma_or_gammas=0.99,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "mse_loss = elwise_mse_loss.sum() / env.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = mse_loss + reg_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.adadelta(loss,weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mean session reward\n",
    "mean_session_reward = env.rewards.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "evaluation_fun = theano.function([],[loss,mse_loss,reg_l2,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session visualization tools\n",
    "\n",
    "Just a helper function that draws current game images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_sessions(max_n_sessions = 3):\n",
    "    \"\"\"just draw random images\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=[15,8])\n",
    "    \n",
    "    pictures = [atari.render(\"rgb_array\") for atari in pool.games[:max_n_sessions]]\n",
    "    for i,pic in enumerate(pictures):\n",
    "        plt.subplot(1,max_n_sessions,i+1)\n",
    "        plt.imshow(pic)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "display_sessions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 25000\n",
    "#25k may take hours to train.\n",
    "#consider interrupt early.\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    update_pool(env,pool,replay_seq_len)\n",
    "    resolver.rng.seed(i)    \n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    if epoch_counter%1 ==0:\n",
    "        current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/1000.)\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%5 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        resolver.epsilon.set_value(0)\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "\n",
    "        print(\"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy))\n",
    "        print(\"rec %.3f reg %.3f\"%(q_loss,l2_penalty))\n",
    "\n",
    "    if epoch_counter %500 ==0:\n",
    "        print(\"Learning curves:\")\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print(\"Random session examples\")\n",
    "        display_sessions()\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Random session examples\")\n",
    "display_sessions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission\n",
    "Here we simply run the OpenAI gym submission code and view scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resolver.epsilon.set_value(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[m.close() for m in gym.monitoring._open_monitors()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "save_path = '/tmp/AgentNet-simplenet-SpaceInvadersv0-Recording0'\n",
    "\n",
    "subm_env = gym.make(GAME_TITLE)\n",
    "\n",
    "#starting monitor. This setup does not write videos\n",
    "#subm_env.monitor.start(save_path,lambda i: False,force=True)\n",
    "\n",
    "#this setup does\n",
    "subm_env.monitor.start(save_path,force=True)\n",
    "\n",
    "\n",
    "for i_episode in xrange(10):\n",
    "    \n",
    "    #initial observation\n",
    "    observation = subm_env.reset()\n",
    "    #initial memory\n",
    "    prev_memories = \"zeros\"\n",
    "    \n",
    "    \n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        action,new_memories = step([observation],prev_memories,batch_size=1)\n",
    "        observation, reward, done, info = subm_env.step(action[0])\n",
    "        \n",
    "        prev_memories = new_memories\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        t+=1\n",
    "\n",
    "subm_env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "gym.upload(save_path,\n",
    "           \n",
    "           #this notebook\n",
    "           writeup=<url to my gist>, \n",
    "           \n",
    "           #your api key\n",
    "           api_key=<my_own_api_key>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Once you got it working,\n",
    "Try building a network that maximizes the final score\n",
    "\n",
    "* Moar lasagne stuff: convolutional layers, batch normalization, nonlinearities and so on\n",
    "* Recurrent agent memory layers, GRUMemoryLayer, etc\n",
    "* Different reinforcement learning algorithm (p.e. qlearning_n_step), other parameters\n",
    "* Experience replay pool\n",
    "\n",
    "\n",
    "Look for examples? Try examples/Deep Kung Fu for most of these features\n",
    "\n",
    "\n",
    "You can also try to expand to a different game: \n",
    " * all OpenAI Atari games are already compatible, you only need to change GAME_TITLE\n",
    " * Other discrete action space environments are also accessible this way\n",
    " * For continuous action spaces, either discretize actions or use continuous RL algorithms (e.g. .learning.dpg_n_step)\n",
    " * Adapting to a custom non-OpenAI environment can be done with a simple wrapper\n",
    " \n",
    " \n",
    "__Good luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
